
1) T/F PCA is used for clustering.  False, principle component analysis is used to reduce dimensionality

2) PCA looks for linear combinations of existing _____.  Columns.  PCA is looking for linear combinations of columns
   	By finding columns that are linear combinations of each other, some can be discarded, reducing dimensionality


3) The "curse of dimensionality" states that the amount of training data needed increases exponentially with the number
   of inputs. They claim this is true, but I would argue it is false since the increase is with the number of columns,
   not the number of inputs.  The word "inputs" seems ill-defined here.

4) X = U * Sigma * V_T    (Sigma here is a diagonal matrix of singular values)

5) Xnorm = (X - mu)/ sigma  (mu is mean, sigma is standard deviation)


6) The function from scipy.linalg that computes svd is scipy.linalg.svd()

7) numpy.allclose() checks that each value of two matrices is within epsilon


